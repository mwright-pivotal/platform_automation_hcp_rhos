{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ba28bd-0f91-43b2-9275-efe187b4f99e",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Goal: Explore a on-prem, RAG solution that is most cost effective in terms of infra requirements and integrate with Vault to protect sensitive information.   Use RHT Hybrid Cloud and HCP Terraform as much as possible to automate full stack starting with bare metal.\n",
    "\n",
    "Source: https://github.com/mwright-pivotal/platform_automation_hcp_rhos \n",
    "\n",
    "References: \n",
    "\n",
    "https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/\n",
    "https://github.com/GURPREETKAURJETHRA/Phi-3-LLM-by-Microsoft/blob/main/Phi3_Testing.ipynb\n",
    "https://python.langchain.com/docs/integrations/vectorstores/pgvector/#instantiation\n",
    "https://www.pragnakalp.com/leverage-phi-3-exploring-rag-based-qna-with-microsofts-phi-3/\n",
    "https://bugbytes.io/posts/vector-databases-pgvector-and-langchain/\n",
    "\n",
    "Archticture:\n",
    "- 3 physical hosts\n",
    "- 2 Nvidia A2 GPUs\n",
    "- Redhat Openshift w/Ceph\n",
    "- NVidia GPU Operator\n",
    "- Postgresql w/PGVector extension (deployed to Openshift)\n",
    "- Developer Workspace using Jupyter allocated to a single GPU (deployed to Openshift)\n",
    "\n",
    "Note: latest version of pgvector not compatible with PGVector from langchain_postgres.vectorstores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8e7ca75-c57a-448d-af5a-354346817b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet openai pandas numpy tiktoken pgvector==0.3.0 python-dotenv einops datasets sentence_transformers flash_attn\n",
    "!pip install --upgrade --quiet  langchain-nvidia-ai-endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a49632a3-97d7-4e92-8ee9-ce0d04b58512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/commands/download.py:132: FutureWarning: Ignoring --local-dir-use-symlinks. Downloading to a local directory does not use symlinks anymore.\n",
      "  warnings.warn(\n",
      "Downloading 'Phi-3-mini-4k-instruct-q4.gguf' to '.cache/huggingface/download/Phi-3-mini-4k-instruct-q4.gguf.8a83c7fb9049a9b2e92266fa7ad04933bb53aa1e85136b7b30f1b8000ff2edef.incomplete'\n",
      "Phi-3-mini-4k-instruct-q4.gguf: 100%|███████| 2.39G/2.39G [00:23<00:00, 102MB/s]\n",
      "Download complete. Moving file to Phi-3-mini-4k-instruct-q4.gguf\n",
      "Phi-3-mini-4k-instruct-q4.gguf\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli download microsoft/Phi-3-mini-4k-instruct-gguf Phi-3-mini-4k-instruct-q4.gguf --local-dir . --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efd6877-90e7-44bc-b2ad-77107ab19f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDACXX=\"/usr/local/cuda-12/bin/nvcc\" CMAKE_ARGS=\"-DGGML_CUDA=on -DCMAKE_CUDA_ARCHITECTURES=all-major\" pip install llama-cpp-python --upgrade --force-reinstall --no-cache-dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef13bf8-acdf-499f-b63c-f3009ebc69ec",
   "metadata": {},
   "source": [
    "# Load text Terraform Docs on GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6f3369f-403d-4a2f-aeb2-575af78c918a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'terraform'...\n",
      "remote: Enumerating objects: 305156, done.\u001b[K\n",
      "remote: Counting objects: 100% (3758/3758), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1816/1816), done.\u001b[K\n",
      "remote: Total 305156 (delta 2221), reused 3182 (delta 1849), pack-reused 301398 (from 1)\u001b[K\n",
      "Receiving objects: 100% (305156/305156), 306.97 MiB | 18.55 MiB/s, done.\n",
      "Resolving deltas: 100% (192729/192729), done.\n",
      "Updating files: 100% (4672/4672), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/hashicorp/terraform\n",
    "\n",
    "!mkdir text-docs\n",
    "!cd terraform/docs; for filename in *.md; do pandoc $filename -f markdown -t plain -o ../../text-docs/$filename.txt; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1b1013b-f6f7-4bc6-9439-64ad1cd257ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "#!git clone https://github.com/hashicorp/tutorials\n",
    "\n",
    "!cd tutorials/content/tutorials; for filename in **/*.mdx; do pandoc $filename -f markdown -t plain -o $filename.txt; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "314a5931-1d07-49b1-b2d8-82095b979bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import tiktoken\n",
    "import ast\n",
    "import pgvector\n",
    "import math\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "df = pd.DataFrame(columns=['title', 'content', 'url'])\n",
    "\n",
    "pathlist = Path('text-docs/').glob('**/*.txt')\n",
    "for path in pathlist:\n",
    "    with open(path) as f: content = f.read()\n",
    "    new_item = {'title': str(path), 'content': content, 'url': 'https://github.com/hashicorp/terraform/tree/main/docs'}\n",
    "    df.loc[len(df.index)] = new_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92693cb-c215-4fb7-988d-292c357ac793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(\"dunzhang/stella_en_1.5B_v5\", trust_remote_code=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c297c117-78f9-4017-ae27-9599e68987ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to help us create the embeddings\n",
    "\n",
    "# Helper func: calculate number of tokens\n",
    "def num_tokens_from_string(string: str, encoding_name = \"cl100k_base\") -> int:\n",
    "    if not string:\n",
    "        return 0\n",
    "    # Returns the number of tokens in a text string\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "# Helper function: calculate length of essay\n",
    "def get_essay_length(essay):\n",
    "    word_list = essay.split()\n",
    "    num_words = len(word_list)\n",
    "    return num_words\n",
    "\n",
    "# Helper function: calculate cost of embedding num_tokens\n",
    "# Assumes we're using the text-embedding-ada-002 model\n",
    "# See https://openai.com/pricing\n",
    "def get_embedding_cost(num_tokens):\n",
    "    return num_tokens/1000*0.0001\n",
    "\n",
    "# Helper function: calculate total cost of embedding all content in the dataframe\n",
    "def get_total_embeddings_cost():\n",
    "    total_tokens = 0\n",
    "    for i in range(len(df.index)):\n",
    "        text = df['content'][i]\n",
    "        token_len = num_tokens_from_string(text)\n",
    "        total_tokens = total_tokens + token_len\n",
    "    total_cost = get_embedding_cost(total_tokens)\n",
    "    return total_cost\n",
    "\n",
    "# Helper function: get embeddings for a text\n",
    "def get_embeddings(text):\n",
    "    #print(text)\n",
    "    # response = client.embeddings.create(\n",
    "    #     model=\"test-embeddings\",\n",
    "    #     input = text.replace(\"\\n\",\" \")\n",
    "    # )\n",
    "    # embedding = response.data[0].embedding\n",
    "\n",
    "    response = llm.create_embedding(input = text.replace(\"\\n\",\" \"))\n",
    "    embedding = response[\"data\"][0][\"embedding\"]\n",
    "    return embedding\n",
    "\n",
    "# Helper function: get embeddings for a text\n",
    "def get_embeddings_hf(text):\n",
    "    passage_embeddings = embedding_model.encode([text])\n",
    "    return passage_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf546537-1c6d-4c63-93e6-5bd5e52ce6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Create new list with small content chunks to not hit max token limits\n",
    "# Note: the maximum number of tokens for a single request is 8191\n",
    "# https://openai.com/docs/api-reference/requests\n",
    "###############################################################################\n",
    "# list for chunked content and embeddings\n",
    "new_list = []\n",
    "# Split up the text into token sizes of around 512 tokens\n",
    "for i in range(len(df.index)):\n",
    "    text = df['content'][i]\n",
    "    token_len = num_tokens_from_string(text)\n",
    "    if token_len <= 512:\n",
    "        new_list.append([df['title'][i], df['content'][i], df['url'][i], token_len])\n",
    "    else:\n",
    "        # add content to the new list in chunks\n",
    "        start = 0\n",
    "        ideal_token_size = 512\n",
    "        # 1 token ~ 3/4 of a word\n",
    "        ideal_size = int(ideal_token_size // (4/3))\n",
    "        end = ideal_size\n",
    "        #split text by spaces into words\n",
    "        words = text.split()\n",
    "\n",
    "        #remove empty spaces\n",
    "        words = [x for x in words if x != ' ']\n",
    "\n",
    "        total_words = len(words)\n",
    "        \n",
    "        #calculate iterations\n",
    "        chunks = total_words // ideal_size\n",
    "        if total_words % ideal_size != 0:\n",
    "            chunks += 1\n",
    "        \n",
    "        new_content = []\n",
    "        for j in range(chunks):\n",
    "            if end > total_words:\n",
    "                end = total_words\n",
    "            new_content = words[start:end]\n",
    "            new_content_string = ' '.join(new_content)\n",
    "            new_content_token_len = num_tokens_from_string(new_content_string)\n",
    "            if new_content_token_len > 0:\n",
    "                new_list.append([df['title'][i], new_content_string, df['url'][i], new_content_token_len])\n",
    "            start += ideal_size\n",
    "            end += ideal_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2bb06a-611e-43b3-a4f8-cbe6f695e6ca",
   "metadata": {},
   "source": [
    "### Option 1: Interacting with PGVector using Langchain VectorStore & HuggingFacePipeline\n",
    "\n",
    "Table creation and structure is defined by the API\n",
    "It expects the Database (eg langchain) to be created beforehand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f5c435f-3320-44ff-a7c6-887f04d6ea62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU langchain_postgres langchain-huggingface langchain langchain-community accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e736217-ba33-4dd8-abb2-956fa427c7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "#model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_name = \"flax-sentence-embeddings/stackoverflow_mpnet-base\"\n",
    "model_kwargs = {\"device\": \"cuda\", \"trust_remote_code\": True}\n",
    "encode_kwargs = {\n",
    "    \"normalize_embeddings\": False,\n",
    "}\n",
    "\n",
    "embeddings_lc = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f6e6d31-b43d-4cb2-a504-e663680662a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "document_list = []\n",
    "pathlist = Path('text-docs/').glob('**/*.txt')\n",
    "for path in pathlist:\n",
    "    loader = TextLoader(path, encoding='utf-8')\n",
    "    documents = loader.load()\n",
    "    document_list+=documents\n",
    "\n",
    "#print(documents)  # prints the document objects\n",
    "print(len(document_list))  # 1 - we've only read one file/document into the loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac82135a-7695-4982-b344-76183eedcbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5514\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "texts = text_splitter.split_documents(document_list)\n",
    "\n",
    "#print(texts)\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "694b1442-ad5b-4da9-afe6-581cc12eb009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: delete tables langchain_pg_collections and langchain_pg_embeddings if you execute this cell more than once\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "# See docker command above to launch a postgres instance with pgvector enabled.\n",
    "pgPassword = \"JuebqFsgss\"\n",
    "connection = \"postgresql+psycopg://postgres:\" + pgPassword + \"@pgvector-postgresql/langchain\"  # Uses psycopg3!\n",
    "collection_name = \"my_docs\"\n",
    "\n",
    "\n",
    "vector_store = PGVector.from_documents(\n",
    "    embedding=embeddings_lc,\n",
    "    documents=texts,\n",
    "    collection_name=collection_name,\n",
    "    connection=connection,\n",
    ")\n",
    "# vector_store = PGVector(\n",
    "#     embeddings=embeddings_lc,\n",
    "#     documents=new_list,\n",
    "#     collection_name=collection_name,\n",
    "#     connection=connection,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "373d9b95-941e-45bf-bbdc-86de596e2592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Document(id='d885d10a-1f7f-43c2-8d19-d90262d03c04', metadata={'source': 'text-docs/kubernetes-scope-microservice.mdx.txt'}, page_content='how it works out'), 0.5332930048354858)\n",
      "\n",
      "(Document(id='dfbb9bcb-1ad8-42d7-834e-edb1443049e9', metadata={'source': 'text-docs/architecture.md.txt'}, page_content='-   NodeDestroyResourceInstance.Execute, which handles the main destroy\\n    operation.\\n\\nA vertex must complete successfully before the graph walk will begin\\nevaluation for other vertices that have “happens after” edges.\\nEvaluation can fail with one or more errors, in which case the graph\\nwalk is halted and the errors are returned to the user.\\n\\nExpression Evaluation\\n\\nAn important part of vertex evaluation for most vertex types is\\nevaluating any expressions in the configuration block associated with\\nthe vertex. This completes the processing of the portions of the\\nconfiguration that were not processed by the configuration loader.\\n\\nThe high-level process for expression evaluation is:'), 0.5630232821994025)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What does an operation consist of?\"\n",
    "similar = vector_store.similarity_search_with_score(query, k=2)\n",
    "\n",
    "for doc in similar:\n",
    "    print(doc, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e049653d-1175-46bb-92f2-869f329689b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility function for answer generation\n",
    "def ask(question):\n",
    "   context = retriever.invoke(question)\n",
    "   #print(context)\n",
    "\n",
    "   answer = (chain({\"input_documents\": context, \"question\": question}, return_only_outputs=True))['output_text']\n",
    "   return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ab7454d-0d66-46a3-80d1-f15866d033c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2005065e5159422d921b58a06902f5b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6913/778192524.py:27: LangChainDeprecationWarning: This class is deprecated. See the following migration guides for replacements based on `chain_type`:\n",
      "stuff: https://python.langchain.com/v0.2/docs/versions/migrating_chains/stuff_docs_chain\n",
      "map_reduce: https://python.langchain.com/v0.2/docs/versions/migrating_chains/map_reduce_chain\n",
      "refine: https://python.langchain.com/v0.2/docs/versions/migrating_chains/refine_chain\n",
      "map_rerank: https://python.langchain.com/v0.2/docs/versions/migrating_chains/map_rerank_docs_chain\n",
      "\n",
      "See also guides on retrieval and question-answering here: https://python.langchain.com/v0.2/docs/how_to/#qa-with-rag\n",
      "  chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "# Define the custom prompt template suitable for the Phi-3 model\n",
    "qna_prompt_template=\"\"\"<|system|>\n",
    "You have been provided with the context and a question, try to find out the answer to the question only using the context information. If the answer to the question is not found within the context, return \"I dont know\" as the response.<|end|>\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "   template=qna_prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "#del llm\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\", device_map='cuda', torch_dtype=\"auto\", trust_remote_code=True,)\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=300)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Define the QNA chain\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs = {\"k\" : 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "720305c0-8672-4fa8-841b-8d53839ae65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  what is waypoint?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: HCP Waypoint is a HashiCorp-managed application deployment platform that simplifies the process of deploying applications into your infrastructure and helps you standardize your deployment process.\n"
     ]
    }
   ],
   "source": [
    "# Take the user input and call the function to generate output\n",
    "user_question = input(\"User: \")\n",
    "answer = ask(user_question)\n",
    "answer = (answer.split(\"<|assistant|>\")[-1]).strip()\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6002682d-f812-44b6-907e-29e17ba8bd6e",
   "metadata": {},
   "source": [
    "### Option 2: Interacting with PGVector with psycopg2 and Llama CPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7b0772-704c-4d30-abb2-bbba843e78ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: binary option works with running in containers...\n",
    "\n",
    "!pip uninstall -y psycopg2\n",
    "!pip install psycopg2-binary --no-binary :all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630feefd-f4d9-4822-8561-8ba0ae61214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(get_embeddings(\"My name is Mike\"))\n",
    "\n",
    "# Create embeddings for each piece of content\n",
    "for i in range(len(new_list)):\n",
    "    text = new_list[i][1]\n",
    "    embedding = get_embeddings_hf(text)\n",
    "    new_list[i].append(embedding)\n",
    "\n",
    "# Create a new dataframe from the list\n",
    "# df_new = pd.DataFrame(new_list, columns=['title', 'content', 'url', 'tokens', 'embeddings'])\n",
    "# df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6520c79-970b-4085-b75c-62532d80fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_embeddings = model.encode(queries, instruction=query_prefix, max_length=max_length)\n",
    "\n",
    "df_new = pd.DataFrame(new_list, columns=['title', 'content', 'url', 'tokens', 'embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0378693-2e41-46b3-9efd-e9be557bcddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pgPassword = \"\"\n",
    "\n",
    "os.environ['PGVECTOR_DB'] = 'postgres://postgres:' + pgPassword + '@pgvector-postgresql'\n",
    "\n",
    "connection_string  = os.environ['PGVECTOR_DB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7809c740-67c3-4f85-a3b6-a786dcc0c46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "from psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "from pgvector.psycopg2 import register_vector\n",
    "\n",
    "llm = Llama(\n",
    "  model_path=\"/home/jovyan/Phi-3-mini-4k-instruct-q4.gguf\",  # path to GGUF file\n",
    "  n_ctx=4096,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "  n_threads=8, # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "  n_gpu_layers=35, # The number of layers to offload to GPU, if you have GPU acceleration available. Set to 0 if no GPU acceleration is available on your system.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553b6daa-fec6-4d67-a315-f817bed1a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = embeddings.embed_query(query)\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6ee723-39a4-44d8-af96-3b6a1557f5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to PostgreSQL database in Timescale using connection string\n",
    "conn = psycopg2.connect(connection_string)\n",
    "cur = conn.cursor()\n",
    "\n",
    "#install pgvector \n",
    "cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector\");\n",
    "conn.commit()\n",
    "\n",
    "# Register the vector type with psycopg2\n",
    "register_vector(conn)\n",
    "\n",
    "table_drop_command = \"\"\"\n",
    "DROP TABLE IF EXISTS embeddings;\n",
    "\"\"\"\n",
    "cur.execute(table_drop_command)\n",
    "\n",
    "# Create table to store embeddings and metadata\n",
    "table_create_command = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS embeddings (\n",
    "            id bigserial primary key, \n",
    "            title text,\n",
    "            url text,\n",
    "            content text,\n",
    "            tokens integer,\n",
    "            embedding vector(1024)\n",
    "            );\n",
    "            \"\"\"\n",
    "\n",
    "cur.execute(table_create_command)\n",
    "cur.close()\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c73fc4-7690-47d7-ab9c-556bd1b62f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_vector(conn)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b1b0fe-41b0-4c79-8afc-4d8137895b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head()['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58625d8c-11a0-4dab-ac1a-03c7115785b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.loc[0]['embeddings'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0621a7-16b9-4627-8625-b27d7aa755fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remind ourselves of the dataframe structure\n",
    "df_temp = df_new.loc[df_new['title'] == 'text-docs/README.md.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078453fc-8543-4c6b-9291-f4654edd9f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe with embeddings as a CSV file\n",
    "df_temp.to_csv('terraformdocs_data_and_embeddings.csv', index=False)\n",
    "# It may also be useful to save as a json file, but we won't use this in the tutorial\n",
    "#df_new.to_json('blog_data_and_embeddings.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eef281-17f2-4d1d-a666-2e23bc7c57f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch insert embeddings and metadata from dataframe into PostgreSQL database\n",
    "# Prepare the list of tuples to insert\n",
    "data_list = [(row['title'], row['url'], row['content'], int(row['tokens']), row['embeddings'][0]) for index, row in df_new.iterrows()]\n",
    "# Use execute_values to perform batch insertion\n",
    "execute_values(cur, \"INSERT INTO embeddings (title, url, content, tokens, embedding) VALUES %s\", data_list)\n",
    "# Commit after we insert all embeddings\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0348334d-6a4d-4efe-940a-ec99cfe1a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT COUNT(*) as cnt FROM embeddings;\")\n",
    "num_records = cur.fetchone()[0]\n",
    "print(\"Number of vector records in table: \", num_records,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e478a10e-ff19-4e0c-86c1-7ef9fff6dee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first record in the table, for sanity-checking\n",
    "cur.execute(\"SELECT * FROM embeddings LIMIT 1;\")\n",
    "records = cur.fetchall()\n",
    "print(\"First record in table: \", records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f702ff5-13fd-4177-abc9-d91fe438d48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index on the data for faster retrieval\n",
    "# this isn't really needed for 129 vectors, but it shows the usage for larger datasets\n",
    "# Note: always create this type of index after you have data already inserted into the DB\n",
    "\n",
    "#calculate the index parameters according to best practices\n",
    "num_lists = num_records / 1000\n",
    "if num_lists < 10:\n",
    "    num_lists = 10\n",
    "if num_records > 1000000:\n",
    "    num_lists = math.sqrt(num_records)\n",
    "\n",
    "#use the cosine distance measure, which is what we'll later use for querying\n",
    "cur.execute(f'CREATE INDEX ON embeddings USING ivfflat (embedding vector_cosine_ops) WITH (lists = {num_lists});')\n",
    "conn.commit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4f8f59-17b7-42bd-b762-e7de169ce696",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "\n",
    "llm = Llama(\n",
    "  model_path=\"./Phi-3-mini-4k-instruct-q4.gguf\",  # path to GGUF file\n",
    "  n_ctx=4096,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "  n_threads=8, # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "  n_gpu_layers=35, # The number of layers to offload to GPU, if you have GPU acceleration available. Set to 0 if no GPU acceleration is available on your system.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b341913c-f8de-406d-a604-84f12930e209",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_prompt = \"Instruct: Given a web search query, retrieve relevant passages that answer the query.\\nQuery: \"\n",
    "queries = [\n",
    "    \"What is Terraform?\",\n",
    "    \"What are the benefits of Terraform?\",\n",
    "]\n",
    "queries = [query_prompt + query for query in queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6487e930-f81f-4893-a4d7-66534d0532fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: Get top 3 most similar documents from the database\n",
    "def get_top3_similar_docs(query_embedding, conn):\n",
    "    embedding_array = np.array(query_embedding)\n",
    "    # Register pgvector extension\n",
    "    register_vector(conn)\n",
    "    cur = conn.cursor()\n",
    "    # Get the top 3 most similar documents using the KNN <=> operator\n",
    "    cur.execute(\"SELECT content FROM embeddings ORDER BY embedding <=> %s LIMIT 3\", (embedding_array,))\n",
    "    top3_docs = cur.fetchall()\n",
    "    return top3_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47815104-2839-43f4-a56d-63e749f2225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question about Timescale we want the model to answer\n",
    "input = \"What is the default planning behavior of terraform?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac44082-39b7-45f9-bd6e-1eb11178c5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process input with retrieval of most similar documents from the database\n",
    "def process_input_with_retrieval(user_input):\n",
    "    delimiter = \"```\"\n",
    "\n",
    "    #Step 1: Get documents related to the user input from database\n",
    "    related_docs = get_top3_similar_docs(get_embeddings_hf(user_input)[0], conn)\n",
    "\n",
    "    # Step 2: Get completion from OpenAI API\n",
    "    # Set system message to help set appropriate tone and context for model\n",
    "    system_message = f\"\"\"\n",
    "    You are a friendly chatbot. \\\n",
    "    You can answer questions about terraform, its features and its use cases. \\\n",
    "    You respond in a concise, technically credible tone. \\\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare messages to pass to model\n",
    "    # We use a delimiter to help the model understand the where the user_input starts and ends\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": f\"{delimiter}{user_input}{delimiter}\"},\n",
    "        {\"role\": \"assistant\", \"content\": f\"Relevant Terraform information: \\n {related_docs[0][0]} \\n {related_docs[1][0]} {related_docs[2][0]}\"}   \n",
    "    ]\n",
    "\n",
    "    # final_response = get_completion_from_messages(messages)\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e431ab5-093d-488e-96a8-290e1549e993",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = process_input_with_retrieval(input)\n",
    "print(input)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ad02f7-f923-47a2-847c-fc58c3cdffcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
